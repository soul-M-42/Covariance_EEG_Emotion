
defaults:
  - _self_
  # - data_1: SEEDV_small
  - data@data_1: SEED
  # - data_1: SEEDV_small
  - data@data_2: SEEDV
  # - data_2: FACED_blink_only_small
  # - data_2: SEEDV_small
  - data@data_val: FACED_blink_only
  # - data_2: FACED_blink_only_small
  # - data_2: SEEDV_small
  # WARNING: REMEMBER TO CHANGE train.n_fold WHEN CHANGE DATASET/DATASET_SMALL
  # SMALL: fold=3
  # NORMAL: fold=16
  - model: cnn_att

seed: 7

channel_encoder:
  patch_size: 32
  hidden_dim: 64
  out_dim: 32
  depth: 4
  patch_stride: 6
  drop_path: 0.1
  n_filter: 16
  filterLen: 10

align:
  device: gpu
  factor: 0.05
  n_channel_uni: 60
  to_riem: False
  proto_loss: False
  align_loss: False
  clisa_loss: True
  MLP_loss: False

log:
  is_logger: True
  logpath: 'log_trinity'
  # logpath: 'log'
  # logpath: 'log'
  run: 4
  proj_name: 'MultiBase'
  # CDA for Cross Dataset Align

  exp_name: $'log.proj_name'

  # cp_dir: ext/mlp cp save path, different dataset will be saved in separate subfolder 
  cp_dir: '/mnt/data/model_weights/DUAL_cp'   

train:
  n_fold: 7
  n_pairs: 1024
  gpus: [0]
  iftest: False
  valid_method: 10
  # n_subs: ${data.n_subs}
  lr: 5e-4
  wd: 0.0001
  loss_temp: 0.07
  max_epochs: 30
  min_epochs: 10
  patience: 30
  restart_times: ${train.max_epochs}  # scheduler
  val_sub_p: 0.8
  grad_accumulation: 1
  num_workers: 0


mlp:
  # fea_dim: 256 #${model.n_timeFilters*model.n_msFilters*4}
  hidden_dim: 128
  out_dim: ${data_val.n_class}
  lr: 0.0005  #0.0005
  wd: 0.0022   #0.001-0.005
  max_epochs: 10
  min_epochs: 2
  patience: 30
  gpus: ${train.gpus}
  num_workers: ${train.num_workers}
  batch_size: 256

ext_fea:
  normTrain: True
  batch_size: 256
  mode: 'me'
  rn_decay: 0.990
  use_pretrain: True


hydra:
  job:
    chdir: false
  run:
    dir: ./clisa_run/${log.proj_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}_run${log.run}